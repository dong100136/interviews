importtensorflowastftf.enable_eager_execution()X=tf.constant([[1.0,2.0,3.0], [4.0,5.0,6.0]])y=tf.constant([[10.0], [20.0]])classLinear(tf.keras.Model):def__init__(self):super().__init__()self.dense=tf.keras.layers.Dense(units=1, kernel_initializer=tf.zeros_initializer(),bias_initializer=tf.zeros_initializer())defcall(self,input):output=self.dense(input)returnoutput#以下代码结构与前节类似model=Linear()optimizer=tf.train.GradientDescentOptimizer(learning_rate=0.01)foriinrange(100):withtf.GradientTape()astape:y_pred=model(X)#调用模型loss=tf.reduce_mean(tf.square(y_pred-y))grads=tape.gradient(loss, model.variables)optimizer.apply_gradients(grads_and_vars=zip(grads, model.variables))print(model.variables)这里，我们没有显式地声明w和b两个变量并写出y_pred = tf.matmul(X, w) + b这一线性变换，而是在初始化部分实例化了一个全连接层（tf.keras.layers.Dense），并在call方法中对这个层进行调用。全连接层封装了output = activation(tf.matmul(input, kernel) + bias)这一线性变换+激活函数的4.1.模型（Model）与层（Layer）15
